import { Component, OnInit, OnDestroy, ViewChild, ElementRef, AfterViewInit, ChangeDetectorRef } from '@angular/core';
import { CommonModule } from '@angular/common';
import { FormsModule } from '@angular/forms';
import { HttpClient } from '@angular/common/http';
import { firstValueFrom } from 'rxjs';
import { AzureOpenAIToolsService } from '../services/azure-openai-tools.service';
import { AuthService } from '../services/auth.service';
import {
  IonHeader,
  IonToolbar,
  IonTitle,
  IonContent,
  IonGrid,
  IonRow,
  IonCol,
  IonIcon,
  IonButton,
  IonButtons,
  IonInput,
  IonCard,
  IonCardContent,
  IonChip,
  IonLabel,
  IonText,
  IonNote,
  IonSpinner,
  IonItem,
  IonBadge,
} from '@ionic/angular/standalone';
import { addIcons } from 'ionicons';
import {
  wifi,
  cloudOffline,
  refresh,
  mic,
  micOutline,
  send,
  chatbubblesOutline,
  volumeHigh,
  playOutline,
  play,
  flash,
  micOff,
  stop,
  stopCircle,
} from 'ionicons/icons';
import { AppHeaderComponent } from '../shared/components/app-header/app-header.component';

interface RealtimeCredentials {
  azure_openai_endpoint: string;
  azure_openai_api_key: string;
  azure_openai_deployment_name: string;
  server_os?: string;
  message: string;
}

interface ChatMessage {
  id: string;
  role: 'user' | 'assistant';
  type: 'text' | 'audio';
  content: string;
  timestamp: string;
  audioBlob?: Blob; // Para reproducir el audio
  transcription?: string; // Transcripci√≥n del audio
  tokens?: number; // Tokens utilizados
  isTranscribing?: boolean; // Si est√° transcribiendo actualmente
}

/**
 * Configuraci√≥n centralizada para Azure OpenAI Realtime API
 * Facilita el mantenimiento y modificaci√≥n de par√°metros en el futuro
 */
interface RealtimeConfig {
  // Configuraci√≥n de API
  apiVersion: string;

  // Configuraci√≥n de sesi√≥n
  modalities: ('text' | 'audio')[];
  instructions: string | ((userName: string) => string); // Puede ser string o funci√≥n que retorna string
  voiceType: 'alloy' | 'echo' | 'shimmer';
  inputAudioFormat: 'pcm16' | 'g711_ulaw' | 'g711_alaw';
  outputAudioFormat: 'pcm16' | 'g711_ulaw' | 'g711_alaw';
  inputAudioTranscription: {
    model: string;
  };
  turnDetection: {
    type: 'server_vad';
    threshold: number;
    prefix_padding_ms: number;
    silence_duration_ms: number;
  };

  // Configuraci√≥n de temperatura
  temperature: number;
  max_response_output_tokens: number;

  // Configuraci√≥n de reconexi√≥n
  maxReconnectAttempts: number;

  // Configuraci√≥n de audio local
  audioSampleRate: number;
  audioChannels: number;
}

@Component({
  selector: 'app-demo-realtime',
  templateUrl: './demo-realtime.page.html',
  styleUrls: ['./demo-realtime.page.scss'],
  standalone: true,
  imports: [
    CommonModule,
    FormsModule,
    IonHeader,
    IonToolbar,
    IonTitle,
    IonContent,
    IonGrid,
    IonRow,
    IonCol,
    IonIcon,
    IonButton,
    IonButtons,
    IonInput,
    IonCard,
    IonCardContent,
    IonChip,
    IonLabel,
    IonText,
    IonNote,
    IonSpinner,
    IonItem,
    IonBadge,
    AppHeaderComponent,
  ],
})
export class DemoRealtimePage implements OnInit, OnDestroy, AfterViewInit {
  @ViewChild('chatContainer') chatContainer!: ElementRef;

  // ==================== CONFIGURACI√ìN CENTRALIZADA ====================

  /**
   * Configuraci√≥n centralizada de Azure OpenAI Realtime API
   * Modificar estos valores seg√∫n necesidades del proyecto
   */
  private readonly realtimeConfig: RealtimeConfig = {
    // API Version - Usar versi√≥n preview disponible para Realtime API
    // Nota: La API de Realtime a√∫n est√° en preview, no hay versi√≥n GA
    apiVersion: '2024-10-01-preview',

    // Modalidades soportadas
    modalities: ['text', 'audio'],

    // Instrucciones del sistema para el asistente
    // Puede ser un string fijo o una funci√≥n que recibe el nombre del usuario
    instructions: (userName: string) =>
      `Eres un asistente √∫til y amigable de Ezekl Budget. Est√°s conversando con ${userName}. ` +
      `Dir√≠gete a la persona por su nombre cuando sea apropiado. ` +
      `Responde de forma concisa, clara y personalizada.`,

    // Tipo de voz para respuestas de audio
    voiceType: 'alloy', // Opciones: 'alloy', 'echo', 'shimmer'

    // Formato de audio de entrada (micr√≥fono del usuario)
    inputAudioFormat: 'pcm16',

    // Formato de audio de salida (respuestas del asistente)
    outputAudioFormat: 'pcm16',

    // Transcripci√≥n autom√°tica del audio de entrada
    inputAudioTranscription: {
      model: 'whisper-1'
    },

    // Detecci√≥n de actividad de voz (VAD) en el servidor
    turnDetection: {
      type: 'server_vad',
      threshold: 0.5,           // Sensibilidad de detecci√≥n (0.0 - 1.0)
      prefix_padding_ms: 300,   // Audio antes de que se detecte habla
      silence_duration_ms: 500  // Silencio para considerar fin de turno
    },

    // Temperatura para generaci√≥n de respuestas (0.0 - 2.0)
    temperature: 0.8,

    // M√°ximo de tokens en respuestas
    max_response_output_tokens: 4096,

    // M√°ximo de intentos de reconexi√≥n
    maxReconnectAttempts: 5,

    // Configuraci√≥n de audio local (Web Audio API)
    audioSampleRate: 24000,     // Hz - debe coincidir con el formato PCM16
    audioChannels: 1            // Mono
  };

  // Estado de conexi√≥n
  isConnected = false;
  isConnecting = false;
  connectionStatusText = 'Desconectado';

  // WebSocket
  private ws: WebSocket | null = null;
  private wsUrl = '';

  // Mensajes
  messages: ChatMessage[] = [];
  messageText = '';
  isAssistantThinking = false;

  // Audio y grabaci√≥n
  isRecording = false;
  isMuted = false;
  isListeningMode = false; // Modo de escucha continua activado
  recordingDuration = 0;
  private recordingInterval: any = null;
  private mediaRecorder: MediaRecorder | null = null;
  private audioChunks: Blob[] = [];
  private audioContext: AudioContext | null = null;
  private audioStream: MediaStream | null = null;
  private audioWorkletNode: AudioWorkletNode | null = null;
  private hasAudioBeenSent = false; // Flag para rastrear si se envi√≥ audio

  // VAD (Voice Activity Detection)
  vadActive = false;
  private vadAnalyser: AnalyserNode | null = null;
  private vadCheckInterval: any = null;
  private vadThreshold = -50; // dB threshold para detectar voz

  // Control de reproducci√≥n y flujo
  isAssistantSpeaking = false;
  isPlayingAudio = false;
  currentPlayingMessageId: string | null = null; // ID del mensaje que se est√° reproduciendo
  private playbackQueue: Blob[] = [];
  private currentAudioSource: AudioBufferSourceNode | null = null;

  // Mapa de audio buffers y datos por mensaje
  private audioBuffersByMessage: Map<string, string[]> = new Map();
  private currentResponseId: string | null = null;
  private currentAssistantMessageId: string | null = null;
  private currentUserMessageId: string | null = null;

  // Sistema de reconexi√≥n
  private reconnectAttempts = 0;
  private isReconnecting = false;

  // Usuario
  private userName: string = 'Usuario';

  constructor(
    private http: HttpClient,
    private cdr: ChangeDetectorRef,
    private azureToolsService: AzureOpenAIToolsService,
    private authService: AuthService
  ) {
    addIcons({
      mic,
      playOutline,
      flash,
      chatbubblesOutline,
      send,
      volumeHigh,
      wifi,
      cloudOffline,
      refresh,
      micOutline,
      play,
      micOff,
      stop,
      stopCircle,
    });
  }

  async ngOnInit() {
    // Cargar nombre del usuario
    this.loadUserName();

    // Conectar a Azure OpenAI Realtime
    await this.connectToRealtime();
  }

  private loadUserName(): void {
    const user = this.authService.currentUser;
    if (user?.nameLogin) {
      this.userName = user.nameLogin;
      console.log('üë§ Usuario identificado para chat realtime:', this.userName);
    }
  }

  ngAfterViewInit() {
    // Scroll autom√°tico al final cuando se agregan mensajes
    setTimeout(() => this.scrollToBottom(), 0);
  }

  ngOnDestroy() {
    this.disconnect();
    this.stopListening();
    if (this.audioStream) {
      this.audioStream.getTracks().forEach(track => track.stop());
    }
  }

  private async connectToRealtime(): Promise<void> {
    try {
      this.isConnecting = true;
      this.connectionStatusText = 'Conectando...';

      // Obtener credenciales del backend
      const credentials = await firstValueFrom(
        this.http.get<RealtimeCredentials>('/api/credentials/realtime')
      );

      // Credenciales obtenidas correctamente

      // Construir URL de WebSocket para Azure OpenAI Realtime API
      // Formato: wss://<endpoint>/openai/realtime?api-version=<version>&deployment=<deployment>&api-key=<key>
      const endpointUrl = new URL(credentials.azure_openai_endpoint);
      const wsProtocol = 'wss:';
      const wsHost = endpointUrl.hostname;

      this.wsUrl = `${wsProtocol}//${wsHost}/openai/realtime?api-version=${this.realtimeConfig.apiVersion}&deployment=${credentials.azure_openai_deployment_name}&api-key=${credentials.azure_openai_api_key}`;

      console.log(`üîå Conectando con API version: ${this.realtimeConfig.apiVersion}`);

      // Crear conexi√≥n WebSocket
      this.ws = new WebSocket(this.wsUrl);

      this.ws.onopen = () => {
        console.log('‚úÖ Conectado a Azure OpenAI Realtime API');
        this.isConnected = true;
        this.isConnecting = false;
        this.reconnectAttempts = 0; // Resetear intentos de reconexi√≥n
        this.isReconnecting = false;
        this.connectionStatusText = 'Conectado';

        // Enviar configuraci√≥n inicial de sesi√≥n
        this.sendSessionConfig();
      };

      this.ws.onmessage = (event) => {
        this.handleRealtimeMessage(event.data);
      };

      this.ws.onerror = (error) => {
        console.error('‚ùå Error WebSocket:', error);
        this.connectionStatusText = 'Error de conexi√≥n';
        this.isConnecting = false;
        this.isConnected = false;
      };

      this.ws.onclose = (event) => {
        console.log('üîå Desconectado de Azure OpenAI Realtime API', event);
        this.isConnected = false;
        this.isConnecting = false;
        this.connectionStatusText = 'Desconectado';

        // Intentar reconexi√≥n autom√°tica si no fue cierre intencional
        if (!event.wasClean && !this.isReconnecting && this.reconnectAttempts < this.realtimeConfig.maxReconnectAttempts) {
          this.attemptReconnection();
        }
      };

    } catch (error) {
      console.error('‚ùå Error conectando:', error);
      this.connectionStatusText = 'Error al conectar';
      this.isConnecting = false;
      this.isConnected = false;
    }
  }

  private sendSessionConfig(): void {
    if (!this.ws || this.ws.readyState !== WebSocket.OPEN) return;

    // Obtener herramientas disponibles del servicio
    const availableTools = this.azureToolsService.getAvailableTools();

    // Obtener instrucciones personalizadas desde la configuraci√≥n
    // Si instructions es una funci√≥n, la ejecutamos con el nombre del usuario
    // Si es un string, lo usamos directamente
    const instructions = typeof this.realtimeConfig.instructions === 'function'
      ? this.realtimeConfig.instructions(this.userName)
      : this.realtimeConfig.instructions;

    // Configurar la sesi√≥n del Realtime API usando la configuraci√≥n centralizada
    const config = {
      type: 'session.update',
      session: {
        modalities: this.realtimeConfig.modalities,
        instructions: instructions, // Usar instrucciones desde config
        voice: this.realtimeConfig.voiceType,
        input_audio_format: this.realtimeConfig.inputAudioFormat,
        output_audio_format: this.realtimeConfig.outputAudioFormat,
        input_audio_transcription: this.realtimeConfig.inputAudioTranscription,
        turn_detection: this.realtimeConfig.turnDetection,
        temperature: this.realtimeConfig.temperature,
        max_response_output_tokens: this.realtimeConfig.max_response_output_tokens,
        tools: availableTools // Agregar herramientas disponibles
      }
    };

    this.ws.send(JSON.stringify(config));
    console.log('‚öôÔ∏è Configuraci√≥n de sesi√≥n enviada con instrucciones personalizadas para:', this.userName);
    console.log(`üîß Herramientas disponibles: ${availableTools.map(t => t.name).join(', ')}`);
  }

  private handleRealtimeMessage(data: string): void {
    try {
      const message = JSON.parse(data);

      switch (message.type) {
        case 'session.created':
          // Sesi√≥n creada
          break;

        case 'session.updated':
          // Sesi√≥n actualizada
          break;

        case 'response.created':
          // Nueva respuesta iniciada - capturar el response_id
          this.currentResponseId = message.response?.id;
          // Response creado
          break;

        case 'response.output_item.added':
          // Nuevo item de salida - crear mensaje de audio del asistente
          if (message.item?.role === 'assistant') {
            this.createAssistantAudioMessage(message.item.id);
          }
          break;

        case 'conversation.item.created':
          // Nuevo item en la conversaci√≥n
          if (message.item?.role === 'assistant') {
            this.isAssistantThinking = true;
          } else if (message.item?.role === 'user') {
            // Crear mensaje del usuario cuando el servidor lo confirma
            const userMessageId = message.item.id;
            this.createUserAudioMessage(userMessageId);
          }
          break;

        case 'conversation.item.input_audio_transcription.completed':
          // Transcripci√≥n del audio del usuario completada
          if (message.item_id && message.transcript) {
            this.updateUserTranscription(message.item_id, message.transcript);
          }
          break;

        case 'response.audio_transcript.delta':
          // Transcripci√≥n parcial del audio de respuesta
          // Transcripci√≥n en progreso
          this.updateAssistantTranscription(message.delta);
          break;

        case 'response.audio_transcript.done':
          // Transcripci√≥n completa del audio
          // Transcripci√≥n completada
          this.finalizeAssistantTranscription();
          break;

        case 'response.text.delta':
          // Texto parcial de respuesta
          this.appendAssistantMessage(message.delta);
          break;

        case 'response.text.done':
          // Respuesta de texto completa
          this.addAssistantMessage(message.text);
          this.isAssistantThinking = false;
          break;

        case 'response.audio.delta':
          // Audio parcial de respuesta - almacenar para reproducci√≥n
          this.storeAudioDelta(message.delta);
          break;

        case 'response.audio.done':
          // Audio completo
          this.finalizeAudioMessage();
          this.isAssistantThinking = false;
          break;

        case 'response.done':
          // Respuesta completa - capturar tokens
          console.log('üì¶ response.done completo:', message);
          if (message.response?.usage) {
            this.updateMessageTokens(message.response.usage);
          }
          this.currentResponseId = null;
          // NO resetear currentAssistantMessageId aqu√≠ para poder actualizar tokens despu√©s
          setTimeout(() => {
            this.currentAssistantMessageId = null;
          }, 100);
          break;

        case 'input_audio_buffer.speech_started':
          // Inicio de habla detectado (VAD)
          this.vadActive = true;
          break;

        case 'input_audio_buffer.speech_stopped':
          // Fin de habla detectado (VAD)
          this.vadActive = false;
          break;

        case 'response.function_call_arguments.delta':
          // Argumentos parciales de funci√≥n
          console.log('üîß Argumentos de funci√≥n (delta):', message);
          break;

        case 'response.function_call_arguments.done':
          // Argumentos completos de funci√≥n - ejecutar la herramienta
          console.log('üîß Llamada a funci√≥n completa:', message);
          this.handleFunctionCall(message);
          break;

        case 'error':
          console.error('‚ùå Error del servidor:', message.error);
          this.isAssistantThinking = false;
          break;
      }
    } catch (error) {
      console.error('‚ùå Error procesando mensaje:', error);
    }
  }

  async sendTextMessage(): Promise<void> {
    if (!this.messageText.trim() || !this.ws || this.ws.readyState !== WebSocket.OPEN) {
      return;
    }

    const text = this.messageText.trim();

    // Agregar mensaje del usuario a la UI
    this.addUserMessage(text, 'text');

    // Enviar al Realtime API
    const event = {
      type: 'conversation.item.create',
      item: {
        type: 'message',
        role: 'user',
        content: [
          {
            type: 'input_text',
            text: text
          }
        ]
      }
    };

    this.ws.send(JSON.stringify(event));

    // Solicitar respuesta
    this.ws.send(JSON.stringify({ type: 'response.create' }));

    this.messageText = '';
    this.isAssistantThinking = true;
  }

  async startListening(): Promise<void> {
    if (!this.isConnected || this.isListeningMode || this.isAssistantSpeaking) return;

    try {
      // Solicitar acceso al micr√≥fono
      this.audioStream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          sampleRate: this.realtimeConfig.audioSampleRate,
          channelCount: this.realtimeConfig.audioChannels
        }
      });

      // Crear AudioContext para procesar audio
      this.audioContext = new AudioContext({ sampleRate: this.realtimeConfig.audioSampleRate });
      const source = this.audioContext.createMediaStreamSource(this.audioStream);

      // Crear procesador de audio para streaming en tiempo real
      await this.audioContext.audioWorklet.addModule(
        URL.createObjectURL(new Blob([this.getAudioWorkletProcessorCode()], { type: 'application/javascript' }))
      );

      this.audioWorkletNode = new AudioWorkletNode(this.audioContext, 'audio-processor');

      // Recibir chunks de audio y enviarlos al servidor
      this.audioWorkletNode.port.onmessage = (event) => {
        if (!this.isMuted && !this.isAssistantSpeaking && this.ws && this.ws.readyState === WebSocket.OPEN) {
          // Convertir Float32Array a PCM16 y enviar
          const pcm16 = this.floatToPCM16(event.data);
          const base64Audio = this.arrayBufferToBase64(pcm16.buffer);

          this.ws.send(JSON.stringify({
            type: 'input_audio_buffer.append',
            audio: base64Audio
          }));

          // Marcar que se ha enviado audio
          if (!this.hasAudioBeenSent) {
            this.hasAudioBeenSent = true;
          }
        }
      };

      source.connect(this.audioWorkletNode);
      this.audioWorkletNode.connect(this.audioContext.destination);

      this.isListeningMode = true;
      this.isMuted = false;
      this.recordingDuration = 0;
      this.hasAudioBeenSent = false; // Resetear el flag al iniciar

      // Contador de duraci√≥n
      this.recordingInterval = setInterval(() => {
        this.recordingDuration++;
      }, 1000);

      // Modo de escucha continua activado

    } catch (error) {
      console.error('‚ùå Error accediendo al micr√≥fono:', error);
      alert('No se pudo acceder al micr√≥fono. Verifica los permisos.');
    }
  }

  toggleMute(): void {
    this.isMuted = !this.isMuted;
  }

  stopListening(): void {
    if (!this.isListeningMode) return;

    // Detener primero el streaming de audio antes de cualquier otra cosa
    this.isMuted = true; // Silenciar inmediatamente para prevenir env√≠o de audio

    // Desconectar AudioWorklet primero para detener el procesamiento
    if (this.audioWorkletNode) {
      this.audioWorkletNode.disconnect();
      this.audioWorkletNode = null;
    }

    // Detener el stream del micr√≥fono
    if (this.audioStream) {
      this.audioStream.getTracks().forEach(track => track.stop());
      this.audioStream = null;
    }

    // Cerrar el AudioContext
    if (this.audioContext) {
      this.audioContext.close();
      this.audioContext = null;
    }

    // Detener el contador
    if (this.recordingInterval) {
      clearInterval(this.recordingInterval);
    }

    // Resetear estados
    this.isListeningMode = false;
    this.isMuted = false;
    this.recordingDuration = 0;

    // NO detener la reproducci√≥n de audio del asistente
    // El stop es solo para detener la escucha del micr√≥fono, no la reproducci√≥n

    // Verificar estado del buffer antes de commit
    if (this.ws && this.ws.readyState === WebSocket.OPEN) {
      // Solo hacer commit si realmente se envi√≥ audio Y el VAD detect√≥ habla
      if (this.hasAudioBeenSent && this.vadActive) {
        // Si se envi√≥ audio y el VAD est√° activo, hacer commit y solicitar respuesta
        this.ws.send(JSON.stringify({ type: 'input_audio_buffer.commit' }));
        this.ws.send(JSON.stringify({ type: 'response.create' }));
      } else {
        // Si NO se cumple alguna condici√≥n, solo limpiar el buffer sin commit
        this.ws.send(JSON.stringify({ type: 'input_audio_buffer.clear' }));
      }
      this.hasAudioBeenSent = false;
    }
  }

  private getAudioWorkletProcessorCode(): string {
    return `
      class AudioProcessor extends AudioWorkletProcessor {
        constructor() {
          super();
          this.bufferSize = 4800; // 200ms a 24kHz
          this.buffer = new Float32Array(this.bufferSize);
          this.bufferIndex = 0;
        }

        process(inputs, outputs, parameters) {
          const input = inputs[0];
          if (input.length > 0) {
            const channelData = input[0];

            for (let i = 0; i < channelData.length; i++) {
              this.buffer[this.bufferIndex++] = channelData[i];

              if (this.bufferIndex >= this.bufferSize) {
                this.port.postMessage(this.buffer.slice(0));
                this.bufferIndex = 0;
              }
            }
          }

          return true;
        }
      }

      registerProcessor('audio-processor', AudioProcessor);
    `;
  }

  private floatToPCM16(float32Array: Float32Array): Int16Array {
    const pcm16 = new Int16Array(float32Array.length);
    for (let i = 0; i < float32Array.length; i++) {
      const s = Math.max(-1, Math.min(1, float32Array[i]));
      pcm16[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
    }
    return pcm16;
  }

  private arrayBufferToBase64(buffer: ArrayBufferLike): string {
    const bytes = new Uint8Array(buffer);
    let binary = '';
    for (let i = 0; i < bytes.byteLength; i++) {
      binary += String.fromCharCode(bytes[i]);
    }
    return btoa(binary);
  }

  private addUserMessage(content: string, type: 'text' | 'audio'): void {
    const message: ChatMessage = {
      id: Date.now().toString() + Math.random(),
      role: 'user',
      type,
      content,
      timestamp: new Date().toISOString()
    };

    this.messages.push(message);
    setTimeout(() => this.scrollToBottom(), 100);
  }

  private createUserAudioMessage(itemId: string): void {
    // Buscar si ya existe un mensaje temporal del usuario
    const tempMessage = this.messages.find(m => m.role === 'user' && !m.id.startsWith('item_'));

    if (tempMessage) {
      // Actualizar el ID temporal con el ID real del servidor
      tempMessage.id = itemId;
      tempMessage.isTranscribing = true;
      this.currentUserMessageId = itemId;
    } else {
      // Crear nuevo mensaje si no existe
      const message: ChatMessage = {
        id: itemId,
        role: 'user',
        type: 'audio',
        content: 'üé§ Mensaje de voz',
        timestamp: new Date().toISOString(),
        isTranscribing: true,
        transcription: ''
      };
      this.messages.push(message);
      this.currentUserMessageId = itemId;
      setTimeout(() => this.scrollToBottom(), 100);
    }
  }

  private updateUserTranscription(itemId: string, transcript: string): void {
    const message = this.messages.find(m => m.id === itemId);
    if (message) {
      message.transcription = transcript;
      message.content = transcript;
      message.isTranscribing = false;
      // Transcripci√≥n del usuario completada
    }
  }

  private addAssistantMessage(content: string): void {
    const message: ChatMessage = {
      id: Date.now().toString() + Math.random(),
      role: 'assistant',
      type: 'text',
      content,
      timestamp: new Date().toISOString()
    };

    this.messages.push(message);
    setTimeout(() => this.scrollToBottom(), 100);
  }

  private appendAssistantMessage(delta: string): void {
    const lastMessage = this.messages[this.messages.length - 1];

    if (lastMessage && lastMessage.role === 'assistant') {
      lastMessage.content += delta;
    } else {
      this.addAssistantMessage(delta);
    }

    setTimeout(() => this.scrollToBottom(), 100);
  }

  private createAssistantAudioMessage(itemId: string): void {
    // Crear mensaje de audio del asistente inmediatamente
    const message: ChatMessage = {
      id: itemId,
      role: 'assistant',
      type: 'audio',
      content: 'üéµ Audio recibido',
      timestamp: new Date().toISOString(),
      isTranscribing: true,
      transcription: ''
    };

    this.messages.push(message);
    this.currentAssistantMessageId = itemId;
    this.audioBuffersByMessage.set(itemId, []);
    setTimeout(() => this.scrollToBottom(), 100);
  }

  private updateAssistantTranscription(delta: string): void {
    if (!this.currentAssistantMessageId) return;

    const message = this.messages.find(m => m.id === this.currentAssistantMessageId);
    if (message) {
      if (!message.transcription) {
        message.transcription = '';
      }
      message.transcription += delta;
    }
  }

  private finalizeAssistantTranscription(): void {
    if (!this.currentAssistantMessageId) return;

    const message = this.messages.find(m => m.id === this.currentAssistantMessageId);
    if (message) {
      message.isTranscribing = false;
      // Reemplazar el contenido con la transcripci√≥n completa
      if (message.transcription) {
        message.content = message.transcription;
      }
    }
  }

  private storeAudioDelta(audioBase64: string): void {
    if (!this.currentAssistantMessageId) return;

    // Marcar que el asistente est√° hablando (pausar escucha)
    this.isAssistantSpeaking = true;

    const buffers = this.audioBuffersByMessage.get(this.currentAssistantMessageId);
    if (buffers) {
      buffers.push(audioBase64);
    }
  }

  private finalizeAudioMessage(): void {
    if (!this.currentAssistantMessageId) return;

    const message = this.messages.find(m => m.id === this.currentAssistantMessageId);
    const buffers = this.audioBuffersByMessage.get(this.currentAssistantMessageId);

    if (message && buffers && buffers.length > 0) {
      // Convertir todos los buffers base64 a un Blob
      const audioBlob = this.base64ArrayToBlob(buffers);
      message.audioBlob = audioBlob;
      // Audio finalizado, reproducir autom√°ticamente

      // Reproducir autom√°ticamente el audio del asistente
      setTimeout(() => {
        this.playAudio(message);
      }, 100);
    } else {
      // Si no hay audio, reactivar escucha inmediatamente
      setTimeout(() => {
        this.isAssistantSpeaking = false;
      }, 500);
    }
  }

  private updateMessageTokens(usage: any): void {
    if (!usage) return;

    const inputTokens = usage.input_tokens || 0;
    const outputTokens = usage.output_tokens || 0;
    const totalTokens = usage.total_tokens || (inputTokens + outputTokens);

    // Actualizar tokens del mensaje del USUARIO (input_tokens)
    if (this.currentUserMessageId && inputTokens > 0) {
      const userMessageIndex = this.messages.findIndex(m => m.id === this.currentUserMessageId);
      if (userMessageIndex !== -1) {
        this.messages[userMessageIndex].tokens = inputTokens;
      }
    }

    // Actualizar tokens del mensaje del ASISTENTE (output_tokens)
    if (this.currentAssistantMessageId && outputTokens > 0) {
      const assistantMessageIndex = this.messages.findIndex(m => m.id === this.currentAssistantMessageId);
      if (assistantMessageIndex !== -1) {
        this.messages[assistantMessageIndex].tokens = outputTokens;
      }
    }

    // Crear una nueva referencia del array para que Angular detecte el cambio
    this.messages = [...this.messages];

    // Forzar m√∫ltiples ciclos de detecci√≥n de cambios
    this.cdr.markForCheck();
    this.cdr.detectChanges();

    // Forzar otro ciclo en el siguiente tick
    setTimeout(() => {
      this.cdr.markForCheck();
      this.cdr.detectChanges();
    }, 0);

    setTimeout(() => {
      this.cdr.markForCheck();
      this.cdr.detectChanges();
    }, 100);
  }

  private base64ArrayToBlob(base64Array: string[]): Blob {
    // Convertir array de base64 a Blob de audio
    const binaryStrings = base64Array.map(b64 => atob(b64));
    const lengths = binaryStrings.map(s => s.length);
    const totalLength = lengths.reduce((a, b) => a + b, 0);

    const uint8Array = new Uint8Array(totalLength);
    let offset = 0;

    for (const binaryString of binaryStrings) {
      for (let i = 0; i < binaryString.length; i++) {
        uint8Array[offset++] = binaryString.charCodeAt(i);
      }
    }

    // PCM16 audio blob
    return new Blob([uint8Array], { type: 'audio/pcm' });
  }

  playAudio(message: ChatMessage): void {
    if (!message.audioBlob || this.isPlayingAudio) return;

    // Marcar que estamos reproduciendo este mensaje espec√≠fico
    this.isPlayingAudio = true;
    this.isAssistantSpeaking = true;
    this.currentPlayingMessageId = message.id;

    // Crear AudioContext para reproducir PCM16
    const audioContext = new AudioContext({ sampleRate: this.realtimeConfig.audioSampleRate });
    const reader = new FileReader();

    reader.onload = async (e) => {
      try {
        const arrayBuffer = e.target?.result as ArrayBuffer;

        // Convertir PCM16 a AudioBuffer
        const audioBuffer = this.pcm16ToAudioBuffer(arrayBuffer, audioContext);

        // Reproducir
        this.currentAudioSource = audioContext.createBufferSource();
        this.currentAudioSource.buffer = audioBuffer;
        this.currentAudioSource.connect(audioContext.destination);

        // Cuando termine la reproducci√≥n
        this.currentAudioSource.onended = () => {
          this.currentAudioSource = null;

          // Esperar 500ms antes de reactivar la escucha para evitar bucles
          setTimeout(() => {
            this.isPlayingAudio = false;
            this.isAssistantSpeaking = false;
            this.currentPlayingMessageId = null;
            this.vadActive = false; // Resetear VAD
            // Resetear el flag de audio para evitar commits vac√≠os
            this.hasAudioBeenSent = false;
          }, 500);
        };

        this.currentAudioSource.start(0);
      } catch (error) {
        console.error('‚ùå Error reproduciendo audio:', error);
        this.isPlayingAudio = false;
        this.isAssistantSpeaking = false;
        this.currentPlayingMessageId = null;
        this.currentAudioSource = null;
      }
    };

    reader.readAsArrayBuffer(message.audioBlob);
  }

  stopAudio(): void {
    if (this.currentAudioSource) {
      try {
        this.currentAudioSource.stop();
        this.currentAudioSource = null;
      } catch (error) {
        console.error('‚ùå Error deteniendo audio:', error);
      }
    }

    // Esperar 500ms antes de reactivar para evitar bucle
    setTimeout(() => {
      this.isPlayingAudio = false;
      this.isAssistantSpeaking = false;
      this.currentPlayingMessageId = null;
      this.vadActive = false; // Resetear VAD tambi√©n aqu√≠
      // Resetear el flag de audio para evitar commits vac√≠os
      this.hasAudioBeenSent = false;
    }, 500);
  }

  private pcm16ToAudioBuffer(arrayBuffer: ArrayBuffer, audioContext: AudioContext): AudioBuffer {
    // Convertir PCM16 a AudioBuffer
    const int16Array = new Int16Array(arrayBuffer);
    const float32Array = new Float32Array(int16Array.length);

    // Normalizar de int16 a float32 [-1, 1]
    for (let i = 0; i < int16Array.length; i++) {
      float32Array[i] = int16Array[i] / 32768.0;
    }

    // Crear AudioBuffer
    const audioBuffer = audioContext.createBuffer(1, float32Array.length, 24000);
    audioBuffer.getChannelData(0).set(float32Array);

    return audioBuffer;
  }

  private playAudioDelta(audioBase64: string): void {
    // Este m√©todo ya no se usa, la l√≥gica est√° en storeAudioDelta
    console.log('üîä Audio delta recibido');
  }

  private scrollToBottom(): void {
    if (this.chatContainer?.nativeElement) {
      const element = this.chatContainer.nativeElement;
      element.scrollTop = element.scrollHeight;
    }
  }

  private disconnect(): void {
    if (this.ws) {
      this.ws.close();
      this.ws = null;
    }
  }

  private async attemptReconnection(): Promise<void> {
    if (this.isReconnecting || this.reconnectAttempts >= this.realtimeConfig.maxReconnectAttempts) {
      console.error('‚ùå M√°ximo de intentos de reconexi√≥n alcanzado');
      this.connectionStatusText = 'Reconexi√≥n fallida';
      return;
    }

    this.isReconnecting = true;
    this.reconnectAttempts++;

    // Backoff exponencial: 2^attempts segundos
    const delayMs = Math.min(1000 * Math.pow(2, this.reconnectAttempts), 30000);

    this.connectionStatusText = `Reconectando (${this.reconnectAttempts}/${this.realtimeConfig.maxReconnectAttempts})...`;
    console.log(`üîÑ Intento de reconexi√≥n ${this.reconnectAttempts}/${this.realtimeConfig.maxReconnectAttempts} en ${delayMs}ms`);

    await new Promise(resolve => setTimeout(resolve, delayMs));

    try {
      await this.connectToRealtime();
    } catch (error) {
      console.error('‚ùå Error en reconexi√≥n:', error);
      this.isReconnecting = false;

      // Intentar de nuevo si no se alcanz√≥ el m√°ximo
      if (this.reconnectAttempts < this.realtimeConfig.maxReconnectAttempts) {
        this.attemptReconnection();
      }
    }
  }

  // ==================== MANEJO DE FUNCTION CALLING ====================

  /**
   * Maneja la llamada a una funci√≥n desde Azure OpenAI
   * Ejecuta la herramienta correspondiente y env√≠a el resultado de vuelta
   */
  private async handleFunctionCall(message: any): Promise<void> {
    const callId = message.call_id;
    const functionName = message.name;
    const argumentsStr = message.arguments;

    console.log(`üîß Funci√≥n llamada: ${functionName}`);
    console.log(`üìù Call ID: ${callId}`);
    console.log(`üìã Argumentos (raw): ${argumentsStr}`);

    try {
      // Parsear argumentos
      const args = JSON.parse(argumentsStr);
      console.log(`‚úÖ Argumentos parseados:`, args);

      // Ejecutar la herramienta
      const result = await this.azureToolsService.executeTool(functionName, args);
      console.log(`üìä Resultado de la herramienta:`, result);

      // Enviar el resultado de vuelta a Azure OpenAI
      const functionOutput = {
        type: 'conversation.item.create',
        item: {
          type: 'function_call_output',
          call_id: callId,
          output: JSON.stringify(result.success ? result.data : { error: result.error })
        }
      };

      console.log(`üì§ Enviando resultado de funci√≥n:`, functionOutput);

      if (this.ws && this.ws.readyState === WebSocket.OPEN) {
        this.ws.send(JSON.stringify(functionOutput));

        // Solicitar una nueva respuesta con el resultado de la funci√≥n
        const createResponse = {
          type: 'response.create'
        };
        this.ws.send(JSON.stringify(createResponse));
        console.log('üîÑ Solicitando nueva respuesta con resultado de funci√≥n');
      }

    } catch (error) {
      console.error('‚ùå Error ejecutando funci√≥n:', error);

      // Enviar error de vuelta
      const errorOutput = {
        type: 'conversation.item.create',
        item: {
          type: 'function_call_output',
          call_id: callId,
          output: JSON.stringify({
            error: error instanceof Error ? error.message : 'Error desconocido'
          })
        }
      };

      if (this.ws && this.ws.readyState === WebSocket.OPEN) {
        this.ws.send(JSON.stringify(errorOutput));

        // Solicitar nueva respuesta incluso con error
        const createResponse = {
          type: 'response.create'
        };
        this.ws.send(JSON.stringify(createResponse));
      }
    }
  }

  formatTime(isoString: string): string {
    return new Date(isoString).toLocaleTimeString('es-ES', {
      hour: '2-digit',
      minute: '2-digit'
    });
  }
}
